"""
Requirements file for Hybrid Efficient nnU-Net Training Pipeline
Optimized for production-ready medical image segmentation
"""

# Core ML/DL frameworks
torch>=2.0.0
torchvision>=0.15.0
numpy>=1.21.0

# Medical image processing
# nibabel>=3.2.0  # For NIfTI file handling
# SimpleITK>=2.1.0  # For DICOM and medical image I/O
# monai>=1.1.0  # Medical imaging AI toolkit

# Training and optimization
tqdm>=4.65.0
tensorboard>=2.10.0
wandb>=0.13.0  # For experiment tracking
matplotlib>=3.5.0
seaborn>=0.11.0

# Image processing and augmentation
scikit-image>=0.19.0
opencv-python>=4.6.0
albumentations>=1.3.0  # Advanced augmentations

# Data handling and utilities
pandas>=1.4.0
h5py>=3.7.0
zarr>=2.12.0  # For efficient array storage
tables>=3.7.0  # For HDF5 support

# Configuration and logging
pyyaml>=6.0
omegaconf>=2.2.0  # Advanced configuration management
hydra-core>=1.2.0  # Configuration framework
rich>=12.0.0  # Beautiful terminal output

# Scientific computing
scipy>=1.8.0
scikit-learn>=1.1.0

# Memory optimization and profiling
psutil>=5.9.0
py3nvml>=0.2.7  # NVIDIA GPU monitoring
memory-profiler>=0.60.0

# Jupyter and development
jupyter>=1.0.0
ipykernel>=6.15.0
ipywidgets>=8.0.0

# Code quality and testing
pytest>=7.1.0
black>=22.0.0
flake8>=5.0.0
mypy>=0.971

# Production deployment
onnx>=1.12.0  # Model export
onnxruntime>=1.12.0  # ONNX inference
fastapi>=0.85.0  # For API deployment
uvicorn>=0.18.0  # ASGI server

# Optional: For mixed precision training on older PyTorch versions
# apex  # NVIDIA Apex (compile from source if needed)

# Optional: For advanced optimizations
# triton>=2.0.0  # For custom CUDA kernels
# flash-attn>=2.0.0  # Flash attention (requires compilation)

# Optional: For distributed training
# deepspeed>=0.7.0
# fairscale>=0.4.0
