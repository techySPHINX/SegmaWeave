# Hybrid Efficient nnU-Net for Medical Image Segmentation ğŸ¥ğŸ§ 

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

## ğŸ¯ Overview

This project implements a **state-of-the-art Hybrid Efficient nnU-Net** for 3D medical image segmentation, combining the best of CNNs and Transformers with production-ready training infrastructure. Designed for research and clinical applications in medical imaging.

### Key Features

- ğŸ§¬ **Hybrid CNN-Transformer Architecture**: Combines local feature extraction (CNNs) with global context modeling (Transformers)
- âš¡ **EfficientNet-style MBConv Blocks**: Memory-efficient inverted residual blocks with Squeeze-Excitation attention
- ğŸ“ **Transfer Learning Ready**: Easy encoder pretraining and fine-tuning capabilities
- ğŸ”¥ **Deep Supervision**: Multi-scale loss computation for better gradient flow
- ğŸš€ **Mixed Precision Training**: 2x faster training with automatic mixed precision (AMP)
- ğŸ“Š **Comprehensive Monitoring**: Weights & Biases integration, detailed logging, and metrics tracking
- ğŸ’¾ **Production-Ready**: Model checkpointing, early stopping, gradient clipping, and memory optimization
- ğŸ§ª **Flexible Configuration**: Easy-to-use config system for quick experiments

---

## ğŸ“ Project Structure

```
cnn/
â”œâ”€â”€ config.py                   # Configuration management (model, training, data, system)
â”œâ”€â”€ model.py                    # Main model architecture (HybridEfficientnnUNet)
â”œâ”€â”€ model_components.py         # Reusable components (blocks, attention, convolutions)
â”œâ”€â”€ losses.py                   # Loss functions (Dice, Focal, Tversky, Combined)
â”œâ”€â”€ trainer.py                  # Training pipeline with mixed precision
â”œâ”€â”€ training_utils.py           # Optimizers, schedulers, metrics, logging
â”œâ”€â”€ train.py                    # Main training script
â”œâ”€â”€ modified1.py                # Experimental/alternative blocks
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ setup.ps1                   # Windows setup script
â”œâ”€â”€ LICENSE                     # MIT License
â”œâ”€â”€ ARCHITECTURE.md             # Detailed architecture documentation
â”œâ”€â”€ API_DOCUMENTATION.md        # API reference guide
â”œâ”€â”€ CONTRIBUTING.md             # Contribution guidelines
â”œâ”€â”€ tests/                      # Unit and integration tests
â”‚   â”œâ”€â”€ test_model.py
â”‚   â”œâ”€â”€ test_losses.py
â”‚   â””â”€â”€ test_training.py
â”œâ”€â”€ docs/                       # Additional documentation
â”‚   â”œâ”€â”€ TRAINING_GUIDE.md
â”‚   â”œâ”€â”€ DEPLOYMENT.md
â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”œâ”€â”€ checkpoints/                # Saved model checkpoints
â”œâ”€â”€ logs/                       # Training logs
â”œâ”€â”€ outputs/                    # Experiment outputs
â”œâ”€â”€ data/                       # Dataset directory
â””â”€â”€ notebooks/                  # Jupyter notebooks for Colab
    â””â”€â”€ HybridnnUNet_Colab.ipynb
```

---

## ğŸ› ï¸ Installation

### Prerequisites

- Python 3.8+
- CUDA 11.0+ (for GPU support)
- 16GB+ RAM recommended
- GPU with 8GB+ VRAM recommended

### Quick Setup

#### Option 1: Using pip (Recommended)

```bash
# Clone the repository
git clone https://github.com/techySPHINX/hybrid-nnunet.git
cd hybrid-nnunet

# Create virtual environment
python -m venv venv

# Activate virtual environment
# Windows PowerShell
.\venv\Scripts\Activate.ps1
# Linux/Mac
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

#### Option 2: Using setup script (Windows)

```powershell
.\setup.ps1
```

### Google Colab Setup

For Google Colab usage, use the provided notebook:

```python
# Upload HybridnnUNet_Colab.ipynb to Colab
# All dependencies will be installed automatically
!pip install torch torchvision torchaudio wandb
```

---

## ğŸš€ Quick Start

### 1. Quick Test (5 epochs, small model)

```bash
python train.py --config quick_test
```

### 2. Production Training (400 epochs, full model)

```bash
python train.py --config production --epochs 400 --batch_size 2 --wandb_project my_project
```

### 3. Fine-tuning from Pretrained Model

```bash
python train.py --config fine_tuning --pretrained checkpoints/best_model.pth
```

### 4. Resume Training from Checkpoint

```bash
python train.py --resume checkpoints/latest_model.pth
```

### 5. Custom Configuration

```bash
python train.py --config production \
    --lr 0.0005 \
    --batch_size 1 \
    --patch_size 128 128 128 \
    --epochs 200
```

---

## ğŸ“Š Model Architecture

### Hybrid Efficient nnU-Net

The model combines:

1. **Encoder**: Progressive feature extraction with MBConv blocks

   - Stem: Initial 3D convolution
   - 4 encoder stages with downsampling
   - Bottleneck with full Transformer attention

2. **Decoder**: Skip connections with upsampling

   - 4 decoder stages
   - Hybrid blocks (CNN + optional Transformer)
   - Deep supervision outputs

3. **Key Components**:
   - **MBConv Blocks**: Inverted residuals with SE attention
   - **Shuffle Attention**: Enhanced feature representation
   - **Multi-Head Self-Attention**: Global context modeling
   - **Depthwise Separable Conv**: Parameter efficiency

### Model Variants

```python
# Lightweight (16M params) - Quick testing
model = create_lightweight_model(num_classes=3)

# Production (20M params) - Best accuracy
model = create_production_model(num_classes=3)

# Large (45M params) - Maximum performance
model = create_large_model(num_classes=3)
```

---

## ğŸ“ˆ Training

### Configuration Presets

#### Quick Test Configuration

- 5 epochs
- Small model (16 base features)
- No Transformer
- Fast validation

#### Production Configuration

- 400 epochs
- Full model (32 base features)
- Hybrid CNN-Transformer
- Mixed precision training
- Deep supervision

#### Fine-tuning Configuration

- 100 epochs
- Lower learning rate (0.0001)
- Pretrained encoder
- 5 warmup epochs

### Custom Configuration

Edit `config.py` or create your own:

```python
from config import Config, ModelConfig, TrainingConfig

model_cfg = ModelConfig(
    base_features=32,
    use_transformer=True,
    dropout_rate=0.1
)

training_cfg = TrainingConfig(
    epochs=400,
    batch_size=2,
    learning_rate=0.001
)

config = Config(model_cfg, training_cfg)
```

---

## ğŸ¯ Expected Results

### Performance Metrics

| Configuration | Dice Score | IoU       | Training Time | GPU Memory |
| ------------- | ---------- | --------- | ------------- | ---------- |
| Quick Test    | 0.65-0.70  | 0.55-0.60 | ~30 min       | 4-6 GB     |
| Production    | 0.85-0.90  | 0.75-0.82 | ~20 hours     | 8-12 GB    |
| Large Model   | 0.88-0.93  | 0.80-0.87 | ~35 hours     | 14-20 GB   |

### Sample Training Curves

- **Epoch 1-50**: Rapid learning phase
- **Epoch 50-200**: Steady improvement
- **Epoch 200-400**: Fine-tuning and convergence

---

## ğŸ§ª Testing & Validation

### Run Unit Tests

```bash
# All tests
pytest tests/

# Specific test
pytest tests/test_model.py -v

# With coverage
pytest --cov=. tests/
```

### Model Validation

```python
from model import create_model
from config import get_production_config

config = get_production_config()
model = create_model(config)

# Test forward pass
import torch
x = torch.randn(1, 4, 128, 128, 128)
output = model(x)
print(f"Output shape: {output.shape}")
```

---

## ğŸ“š Documentation

- [Architecture Documentation](docs/ARCHITECTURE.md) - Detailed model architecture
- [API Reference](docs/API_DOCUMENTATION.md) - Complete API guide
- [Training Guide](docs/TRAINING_GUIDE.md) - Advanced training tips
- [Deployment Guide](docs/DEPLOYMENT.md) - Production deployment
- [Troubleshooting](docs/TROUBLESHOOTING.md) - Common issues & solutions

---

## ğŸ¤ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Development Setup

```bash
# Install dev dependencies
pip install -r requirements-dev.txt

# Install pre-commit hooks
pre-commit install

# Run tests
pytest tests/

# Format code
black .
isort .
```

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- Based on nnU-Net architecture by Isensee et al.
- EfficientNet blocks inspired by Tan & Le
- Transformer components from Vision Transformer (ViT)
- Shuffle Attention from SA-Net paper

---

## ğŸ“ Contact & Support

- **Author**: techySPHINX
- **GitHub**: [techySPHINX/hybrid-nnunet](https://github.com/techySPHINX)
- **Issues**: [GitHub Issues](https://github.com/techySPHINX/hybrid-nnunet/issues)

---

## ğŸ”– Citation

If you use this code in your research, please cite:

```bibtex
@software{hybrid_nnunet_2025,
  author = {techySPHINX},
  title = {Hybrid Efficient nnU-Net for Medical Image Segmentation},
  year = {2025},
  url = {https://github.com/techySPHINX/hybrid-nnunet}
}
```

---

## ğŸ“ Research Applications

This implementation is suitable for:

- ğŸ§  Brain tumor segmentation (BraTS)
- ğŸ« Lung nodule detection
- ğŸ«€ Cardiac structure segmentation
- ğŸ¦´ Bone segmentation
- ğŸ”¬ Multi-organ segmentation
- ğŸ“Š Any 3D medical imaging task

## ğŸ’¡ Tips

- Use GPU for faster training (if available)
- Experiment with hyperparameters
- Try different architectures
- Add data augmentation for better results

## ğŸ“ Requirements

- Python 3.8+
- PyTorch 2.0+
- CUDA (optional, for GPU acceleration)

## ğŸ“ Learning Objectives

- Understanding CNN architecture
- Image classification workflow
- PyTorch fundamentals
- Model training and evaluation
- Data visualization

---

**Happy Learning! ğŸ‰**
